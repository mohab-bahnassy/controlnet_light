# -*- coding: utf-8 -*-
"""controlnet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r8gfsacniz3S6y5Y8FdAnqgGcdwlB-Gc
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

import kagglehub
import os
import shutil

# Download latest version
path = kagglehub.dataset_download("dhananjayapaliwal/fulldataset")

print("Path to dataset files:", path)

import os

directory_path = "/root/.cache/kagglehub/datasets/dhananjayapaliwal/fulldataset/versions/3/temp_extraction/256x256"

folders = [f for f in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, f))]

for folder in folders:
    print(folder)

import os

directory_path = "/kaggle/input/fulldataset/temp_extraction/256x256/splitted_sketches/test"

folders = [f for f in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, f))]

for folder in folders:
    print(folder)

# ============================================================
# ControlNet (Scribble) + Stable Diffusion v1-5
# ============================================================

# 1. Install Dependencies
!pip install -q diffusers transformers accelerate controlnet_aux xformers

# 2. Imports
import torch
from PIL import Image
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
from diffusers.utils import load_image
from controlnet_aux import HEDdetector
import os

# 3. Load Pretrained Models
print("Loading models...")

controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/sd-controlnet-scribble",
    torch_dtype=torch.float16
)

pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    controlnet=controlnet,
    safety_checker=None,
    torch_dtype=torch.float16
)

# Optimizations
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_xformers_memory_efficient_attention()
pipe.enable_model_cpu_offload()

# Load new Annotator repo
print("Loading HED Detector (lllyasviel/Annotators)...")
hed_detector = HEDdetector.from_pretrained("lllyasviel/Annotators")

# 4. Load Input Image
image_url = "https://huggingface.co/lllyasviel/sd-controlnet-scribble/resolve/main/images/bag.png"
input_image = load_image(image_url).resize((512, 512))
input_image.show()

# 5. Generate Scribble Conditioning Map
print("Generating scribble conditioning map...")
control_image = hed_detector(input_image, scribble=True)
control_image.show()

# 6. Prompts and Settings
PROMPT = "A realistic photo of a luxury leather handbag on a table, studio lighting, 8k"
NEGATIVE_PROMPT = "low quality, deformed, extra limbs, blurry, bad lighting"
NUM_STEPS = 25
GUIDANCE_SCALE = 9.0
CONTROLNET_SCALE = 1.0
SEED = 1234

generator = torch.Generator(device="cuda").manual_seed(SEED)

# 7. Run Inference
print("Running inference...")
result = pipe(
    prompt=PROMPT,
    negative_prompt=NEGATIVE_PROMPT,
    image=control_image,
    controlnet_conditioning_scale=CONTROLNET_SCALE,
    num_inference_steps=NUM_STEPS,
    guidance_scale=GUIDANCE_SCALE,
    generator=generator
).images[0]

result.show()

# 8. Display Results
def image_grid(imgs, rows, cols):
    assert len(imgs) == rows * cols
    w, h = imgs[0].size
    grid = Image.new('RGB', size=(cols * w, rows * h))
    for i, img in enumerate(imgs):
        grid.paste(img, box=(i % cols * w, i // cols * h))
    return grid

grid = image_grid([input_image, control_image, result], 1, 3)
grid.show()

# Save results
os.makedirs("outputs", exist_ok=True)
grid.save("outputs/scribble_result_grid.png")
print("Saved result grid to outputs/scribble_result_grid.png")

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/bioinf-jku/TTUR.git
# %cd TTUR

# 9. Calculate statistics for the sketchy dataset
# Source: https://github.com/bioinf-jku/TTUR/blob/master/precalc_stats_example.py
#!/usr/bin/env python3
import os
import glob
import numpy as np
import fid
from imageio.v2 import imread
# LLM used to correct compatibility issue
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

########
# PATHS
########
data_path = '/content/drive/Shareddrives/AML/Sketchy/Rendered Images/256x256/photo' # set path to training set images
output_path = '/content/drive/Shareddrives/AML/Sketchy/fid_stats.npz' # path for where to store the statistics
# if you have downloaded and extracted
#   http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz
# set this path to the directory where the extracted files are, otherwise
# just set it to None and the script will later download the files for you
inception_path = None
print("check for inception model..", end=" ", flush=True)
inception_path = fid.check_or_download_inception(inception_path) # download inception if necessary
print("ok")

# loads all images into memory (this might require a lot of RAM!)
print("load images..", end=" " , flush=True)
# LLM used to handle recursive file reading
image_list = glob.glob(os.path.join(data_path, '**', '*.jpg'), recursive=True)
images = np.array([imread(str(fn)).astype(np.float32) for fn in image_list])
print("%d images found and loaded" % len(images))

print("create inception graph..", end=" ", flush=True)
fid.create_inception_graph(inception_path)  # load the graph into the current TF graph
print("ok")

print("calculte FID stats..", end=" ", flush=True)
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    mu, sigma = fid.calculate_activation_statistics(images, sess, batch_size=100)
    np.savez_compressed(output_path, mu=mu, sigma=sigma)
print("finished")

# Example using bash inside Colab
!7z x "/content/drive/Shareddrives/AML/splitted_sketches.zip" -o"/content/drive/Shareddrives/AML/Sketchy/Rendered Images/256x256/splitted sketches"

import os
import random

photo_root = "/kaggle/input/fulldataset/temp_extraction/256x256/photo"
output_dir = "/kaggle/working"

image_paths = []
for folder in os.listdir(photo_root):
    folder_path = os.path.join(photo_root, folder)
    if os.path.isdir(folder_path):
        for class_name in os.listdir(folder_path):
            class_dir = os.path.join(folder_path, class_name)
            if os.path.isdir(class_dir):
                for fname in os.listdir(class_dir):
                    full_path = os.path.join(class_dir, fname)
                    if os.path.isfile(full_path):
                        image_paths.append(full_path)
                    print("folder finished")

random.seed(42)
random.shuffle(image_paths)
split_idx = int(0.8 * len(image_paths))
train_paths = image_paths[:split_idx]
test_paths = image_paths[split_idx:]

with open(os.path.join(output_dir, "train.txt"), "w") as f_train:
    for path in train_paths:
        f_train.write(path + "\n")

with open(os.path.join(output_dir, "test.txt"), "w") as f_test:
    for path in test_paths:
        f_test.write(path + "\n")

import pandas as pd

# Load the CSV file
csv_path = "/content/drive/Shareddrives/AML/path_caption_pairs.csv"
df = pd.read_csv(csv_path)

# Update the path in the 'image_path' column, keeping part from '256x256' onwards
new_base_path = "/content/drive/Shareddrives/AML/Sketchy/Rendered Images/"
def update_path(x):
    idx = x.find("256x256")
    return new_base_path + x[idx:] if idx != -1 else new_base_path + x.split('/')[-1]

df['image_path'] = df['image_path'].apply(update_path)

# Save the updated DataFrame to CSV
df.to_csv(csv_path, index=False)

import pandas as pd
import os

def load_captions_dict_filename_folder(csv_path):
    df = pd.read_csv(csv_path)
    captions_dict = {}

    for _, row in df.iterrows():
        file_path = row['image_path']
        caption = row['caption']
        parts = file_path.replace("\\", "/").split("/")
        parent = parts[-2]
        filename = os.path.splitext(parts[-1])[0]
        filename_main = filename.split('-')[0]  # Remove trailing index like '-1'
        key = f"{parent}/{filename_main}"
        captions_dict[key] = caption

    return captions_dict

# Example usage:
csv_path = "/content/drive/Shareddrives/AML/path_caption_pairs.csv"
captions = load_captions_dict_filename_folder(csv_path)


# To access caption:
key = "airplane/n02691156_10151"   # example with immediate parent and filename
print(captions[key])

def extract_key_from_sketch_path(sketch_path):
    parts = sketch_path.replace("\\", "/").split("/")
    parent = parts[-2]  # 'ant'
    filename = os.path.splitext(parts[-1])[0]  # 'n02219486_11726-1'
    # Remove trailing dash and digits (e.g., "-1", "-12")
    filename_main = filename.split('-')[0]
    return f"{parent}/{filename_main}"

import os
from PIL import Image
import torch
import json


def generate_images_batched(photo_root, sketch_root, output_root,
                            captions_dict,
                            NEGATIVE_PROMPT,
                            NUM_STEPS, GUIDANCE_SCALE, CONTROLNET_SCALE, SEED,
                            batch_size=64,
                            checkpoint_file="checkpoint.json"):
    os.makedirs(output_root, exist_ok=True)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load checkpoint if exists
    processed_files = set()
    if os.path.exists(checkpoint_file):
        with open(checkpoint_file, 'r') as f:
            checkpoint_data = json.load(f)
            processed_files = set(checkpoint_data.get('processed_files', []))
        print(f"Resuming from checkpoint. Already processed {len(processed_files)} files.")

    # List to hold batch inputs
    batch_sketch_imgs = []
    batch_prompts = []
    batch_output_paths = []
    batch_sketch_paths = []  # Track file paths for checkpoint

    def save_checkpoint():
        """Save current progress to checkpoint file"""
        with open(checkpoint_file, 'w') as f:
            json.dump({'processed_files': list(processed_files)}, f)
        print(f"Checkpoint saved. Processed {len(processed_files)} files so far.")

    def process_batch():
        if not batch_sketch_imgs:
            return

        print(f"Running inference on batch of {len(batch_sketch_imgs)} images...")

        # Run inference on batch
        results = pipe(
            prompt=batch_prompts,
            negative_prompt=[NEGATIVE_PROMPT]*len(batch_prompts),
            image=batch_sketch_imgs,
            controlnet_conditioning_scale=CONTROLNET_SCALE,
            num_inference_steps=NUM_STEPS,
            guidance_scale=GUIDANCE_SCALE,
            generator=torch.Generator(device=device).manual_seed(SEED)
        ).images

        # Save all results in batch
        for res_img, out_path, sketch_path in zip(results, batch_output_paths, batch_sketch_paths):
            res_img.save(out_path)
            processed_files.add(sketch_path)
            print(f"Saved generated image to {out_path}")

        # Save checkpoint after each batch
        save_checkpoint()

        # Clear batches
        batch_sketch_imgs.clear()
        batch_prompts.clear()
        batch_output_paths.clear()
        batch_sketch_paths.clear()

    total_skipped = 0
    total_processed = 0

    for root, _, files in os.walk(sketch_root):
        for sketch_file in files:
            if not sketch_file.lower().endswith(('.jpg', '.jpeg', '.png')):
                continue

            sketch_path = os.path.join(root, sketch_file)

            # Skip if already processed
            if sketch_path in processed_files:
                total_skipped += 1
                continue

            key = extract_key_from_sketch_path(sketch_path)

            if key not in captions_dict:
                print(f"No caption for key {key}, skipping.")
                continue

            caption = captions_dict[key]

            sketch_img = Image.open(sketch_path).convert("RGB").resize((512, 512))

            # Generate control image conditioning
            control_image = hed_detector(sketch_img, scribble=True)

            # Add to batch
            print(f"Processing sketch: {sketch_file}")
            print(f"Key: {key}")
            print(f"Caption: {caption}")

            batch_sketch_imgs.append(control_image)
            batch_prompts.append(caption)
            batch_sketch_paths.append(sketch_path)

            rel_path = os.path.relpath(root, sketch_root)
            out_dir = os.path.join(output_root, rel_path)
            os.makedirs(out_dir, exist_ok=True)
            out_path = os.path.join(out_dir, sketch_file)
            batch_output_paths.append(out_path)

            # Process batch if full
            if len(batch_sketch_imgs) >= batch_size:
                process_batch()
                total_processed += len(batch_sketch_imgs)

    # Process remaining images
    if batch_sketch_imgs:
        total_processed += len(batch_sketch_imgs)
        process_batch()

    print(f"All batches processed!")
    print(f"Total processed: {len(processed_files)}")
    print(f"Skipped (already done): {total_skipped}")
    print(f"Newly processed: {total_processed}")


# Example loading captions dictionary and running:

csv_path = "/content/drive/Shareddrives/AML/path_caption_pairs.csv"
captions = load_captions_dict_filename_folder(csv_path)
generate_images_batched(
    photo_root="/root/.cache/kagglehub/datasets/dhananjayapaliwal/fulldataset/versions/3/temp_extraction/256x256/splitted_sketches/test/photo",
    sketch_root="/root/.cache/kagglehub/datasets/dhananjayapaliwal/fulldataset/versions/3/temp_extraction/256x256/splitted_sketches/test",
    output_root="/content/drive/MyDrive/AML",
    captions_dict=captions,
    NEGATIVE_PROMPT="low quality, deformed, extra limbs, blurry, bad lighting",
    NUM_STEPS=25,
    GUIDANCE_SCALE=9.0,
    CONTROLNET_SCALE=1.0,
    SEED=1234,
    checkpoint_file="/content/drive/MyDrive/AML/checkpoint.json"  # Save to Drive
)

import os
from PIL import Image
import torch
import torchvision.transforms as T
import json
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache


def generate_images_batched(photo_root, sketch_root, output_root,
                            captions_dict,
                            NEGATIVE_PROMPT,
                            NUM_STEPS, GUIDANCE_SCALE, CONTROLNET_SCALE, SEED,
                            batch_size=64,
                            checkpoint_file="checkpoint.json",
                            preload_workers=4):
    os.makedirs(output_root, exist_ok=True)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # GPU-accelerated image transforms
    gpu_transform = T.Compose([
        T.ToTensor(),
        T.Resize((512, 512), antialias=True)
    ])

    # Move hed_detector to GPU if it's a model
    try:
        hed_detector.to(device)
        print("HED detector moved to GPU")
    except:
        print("HED detector not moved to GPU (may not be a torch model)")

    # Load checkpoint if exists
    processed_files = set()
    if os.path.exists(checkpoint_file):
        with open(checkpoint_file, 'r') as f:
            checkpoint_data = json.load(f)
            processed_files = set(checkpoint_data.get('processed_files', []))
        print(f"Resuming from checkpoint. Already processed {len(processed_files)} files.")

    # List to hold batch inputs
    batch_sketch_imgs = []
    batch_prompts = []
    batch_output_paths = []
    batch_sketch_paths = []

    def save_checkpoint():
        """Save current progress to checkpoint file"""
        with open(checkpoint_file, 'w') as f:
            json.dump({'processed_files': list(processed_files)}, f)
        print(f"Checkpoint saved. Processed {len(processed_files)} files so far.")

    def load_and_preprocess_gpu(sketch_path, key):
        """Load and preprocess a single sketch image using GPU (runs in parallel)"""
        try:
            # Load image on CPU
            sketch_img = Image.open(sketch_path).convert("RGB")

            # Transform to tensor and resize on GPU
            with torch.no_grad():
                img_tensor = gpu_transform(sketch_img).to(device)

                # Check if hed_detector accepts tensors
                try:
                    # Try passing tensor directly to hed_detector
                    control_image = hed_detector(img_tensor.unsqueeze(0), scribble=True)
                    # If it returns a tensor, keep on GPU
                    if isinstance(control_image, torch.Tensor):
                        return control_image.squeeze(0), key
                except:
                    # If hed_detector needs PIL, convert back
                    sketch_img_resized = T.ToPILImage()(img_tensor.cpu())
                    control_image = hed_detector(sketch_img_resized, scribble=True)
                    return control_image, key

        except Exception as e:
            print(f"Error processing {sketch_path}: {e}")
            return None, None

    def process_batch():
        if not batch_sketch_imgs:
            return

        print(f"Running inference on batch of {len(batch_sketch_imgs)} images...")

        # Ensure all images are on the correct device
        batch_images_gpu = []
        for img in batch_sketch_imgs:
            if isinstance(img, torch.Tensor):
                batch_images_gpu.append(img.to(device))
            else:
                batch_images_gpu.append(img)

        # Run inference on batch with GPU
        with torch.cuda.amp.autocast():  # Use automatic mixed precision for speed
            results = pipe(
                prompt=batch_prompts,
                negative_prompt=[NEGATIVE_PROMPT]*len(batch_prompts),
                image=batch_images_gpu,
                controlnet_conditioning_scale=CONTROLNET_SCALE,
                num_inference_steps=NUM_STEPS,
                guidance_scale=GUIDANCE_SCALE,
                generator=torch.Generator(device=device).manual_seed(SEED)
            ).images

        # Save all results in batch
        for res_img, out_path, sketch_path in zip(results, batch_output_paths, batch_sketch_paths):
            res_img.save(out_path)
            processed_files.add(sketch_path)
            print(f"Saved generated image to {out_path}")

        # Save checkpoint after each batch
        save_checkpoint()

        # Clear batches and free GPU memory
        batch_sketch_imgs.clear()
        batch_prompts.clear()
        batch_output_paths.clear()
        batch_sketch_paths.clear()
        torch.cuda.empty_cache()  # Free unused GPU memory

    total_skipped = 0
    total_processed = 0

    # Collect all files first (faster than processing during walk)
    print("Collecting sketch files...")
    sketch_files_to_process = []

    for root, _, files in os.walk(sketch_root):
        for sketch_file in files:
            if not sketch_file.lower().endswith(('.jpg', '.jpeg', '.png')):
                continue

            sketch_path = os.path.join(root, sketch_file)

            # Skip if already processed
            if sketch_path in processed_files:
                total_skipped += 1
                continue

            key = extract_key_from_sketch_path(sketch_path)

            if key not in captions_dict:
                continue

            rel_path = os.path.relpath(root, sketch_root)
            out_dir = os.path.join(output_root, rel_path)
            os.makedirs(out_dir, exist_ok=True)
            out_path = os.path.join(out_dir, sketch_file)

            sketch_files_to_process.append({
                'sketch_path': sketch_path,
                'key': key,
                'caption': captions_dict[key],
                'out_path': out_path
            })

    print(f"Found {len(sketch_files_to_process)} files to process")

    # Process files in batches with parallel loading
    with ThreadPoolExecutor(max_workers=preload_workers) as executor:
        for i in range(0, len(sketch_files_to_process), batch_size):
            batch_files = sketch_files_to_process[i:i+batch_size]

            print(f"Preprocessing batch {i//batch_size + 1}/{(len(sketch_files_to_process)-1)//batch_size + 1}...")

            # Load and preprocess images in parallel with GPU
            futures = [
                executor.submit(load_and_preprocess_gpu, file_info['sketch_path'], file_info['key'])
                for file_info in batch_files
            ]

            # Collect results
            for future, file_info in zip(futures, batch_files):
                control_image, key = future.result()

                if control_image is None:
                    continue

                batch_sketch_imgs.append(control_image)
                batch_prompts.append(file_info['caption'])
                batch_sketch_paths.append(file_info['sketch_path'])
                batch_output_paths.append(file_info['out_path'])

            # Process the batch
            if batch_sketch_imgs:
                total_processed += len(batch_sketch_imgs)
                process_batch()

    print(f"All batches processed!")
    print(f"Total processed: {len(processed_files)}")
    print(f"Skipped (already done): {total_skipped}")
    print(f"Newly processed: {total_processed}")


# Example usage:
csv_path = "/content/drive/Shareddrives/AML/path_caption_pairs.csv"
captions = load_captions_dict_filename_folder(csv_path)
generate_images_batched(
    photo_root="/kaggle/input/fulldataset/temp_extraction/256x256/photo",
    sketch_root="/kaggle/input/fulldataset/temp_extraction/256x256/splitted_sketches/test",
    output_root="/content/drive/MyDrive/AML/Generated Images",
    captions_dict=captions,
    NEGATIVE_PROMPT="low quality, deformed, extra limbs, blurry, bad lighting",
    NUM_STEPS=25,
    GUIDANCE_SCALE=9.0,
    CONTROLNET_SCALE=1.0,
    SEED=1234,
    checkpoint_file="/content/drive/MyDrive/AML/checkpoint.json",
    preload_workers=4
)

import os
import shutil

# Define the source and destination directories
source_dir = "/content/drive/MyDrive/AML/Generated Images"
destination_dir = "/content/drive/MyDrive/AML/Flattened Images"

# Create the destination directory if it doesn't exist
os.makedirs(destination_dir, exist_ok=True)

# Walk through the source directory and copy each file
for root, _, files in os.walk(source_dir):
    for file in files:
        source_path = os.path.join(root, file)
        destination_path = os.path.join(destination_dir, file)
        shutil.copy2(source_path, destination_path)

print(f"All images from {source_dir} have been copied to {destination_dir}")

pip install pytorch-fid

!python -m pytorch_fid /content/drive/MyDrive/AML/fid_stats.npz "/content/drive/MyDrive/AML/Flattened Generated Images V4"

pip install clip-score

!python -m clip_score "/content/drive/MyDrive/AML/Flattened Generated Images V4" "/content/drive/MyDrive/AML/Flattened Generated Images Captions"

import os
import pandas as pd

def load_captions_dict_filename_folder(csv_path):
    """Load captions from CSV into a dictionary"""
    df = pd.read_csv(csv_path)
    captions_dict = {}

    for _, row in df.iterrows():
        file_path = row['image_path']
        caption = row['caption']
        parts = file_path.replace("\\", "/").split("/")
        parent = parts[-2]
        filename = os.path.splitext(parts[-1])[0]
        filename_main = filename.split('-')[0]  # Remove trailing index like '-1'
        key = f"{parent}/{filename_main}"
        captions_dict[key] = caption

    return captions_dict


def extract_key_from_image_path(image_path):
    """Extract key from generated image path (same as sketch path logic)"""
    parts = image_path.replace("\\", "/").split("/")
    parent = parts[-2]  # category folder
    filename = os.path.splitext(parts[-1])[0]  # filename without extension
    # Remove trailing dash and digits (e.g., "-1", "-12")
    filename_main = filename.split('-')[0]
    return f"{parent}/{filename_main}"


def create_caption_txt_files(generated_images_root, output_txt_root, captions_dict):
    """
    Create a .txt file for each generated image with its corresponding caption.

    Args:
        generated_images_root: Root directory containing generated images
        output_txt_root: Root directory where .txt files will be saved
        captions_dict: Dictionary mapping keys to captions
    """
    os.makedirs(output_txt_root, exist_ok=True)

    created_count = 0
    missing_count = 0
    missing_files = []

    # Walk through all generated images
    for root, _, files in os.walk(generated_images_root):
        for img_file in files:
            # Only process image files
            if not img_file.lower().endswith(('.jpg', '.jpeg', '.png')):
                continue

            img_path = os.path.join(root, img_file)

            # Extract key from image path
            key = extract_key_from_image_path(img_path)

            # Check if caption exists
            if key not in captions_dict:
                missing_count += 1
                missing_files.append((img_path, key))
                print(f"Warning: No caption found for key '{key}' (file: {img_path})")
                continue

            caption = captions_dict[key]

            # Create corresponding directory structure in output
            rel_path = os.path.relpath(root, generated_images_root)
            out_dir = os.path.join(output_txt_root, rel_path)
            os.makedirs(out_dir, exist_ok=True)

            # Create .txt file with same name as image
            txt_filename = os.path.splitext(img_file)[0] + ".txt"
            txt_path = os.path.join(out_dir, txt_filename)

            # Write caption to file
            with open(txt_path, 'w', encoding='utf-8') as f:
                f.write(caption)

            created_count += 1

            if created_count % 100 == 0:
                print(f"Created {created_count} caption files...")

    print(f"\n{'='*60}")
    print(f"Caption file creation complete!")
    print(f"Total .txt files created: {created_count}")
    print(f"Missing captions: {missing_count}")

    if missing_files:
        print(f"\nFirst 10 missing caption keys:")
        for img_path, key in missing_files[:10]:
            print(f"  Key: '{key}' - File: {img_path}")

    return created_count, missing_count

# Load captions from CSV
csv_path = "/content/drive/Shareddrives/AML/path_caption_pairs.csv"
print("Loading captions from CSV...")
captions = load_captions_dict_filename_folder(csv_path)
print(f"Loaded {len(captions)} captions")

# Create .txt files for generated images
generated_images_root = "/content/drive/MyDrive/AML/Generated Images"
output_txt_root = "/content/drive/MyDrive/AML/Generated Images Captions"

print(f"\nCreating caption .txt files...")
print(f"Source: {generated_images_root}")
print(f"Output: {output_txt_root}\n")

create_caption_txt_files(generated_images_root, output_txt_root, captions)

import shutil
def flatten_folder(source_root, output_flat_folder, file_extensions=None):
    """
    Flatten a nested folder structure into a single folder.

    Args:
        source_root: Root directory with nested structure to flatten
        output_flat_folder: Single folder where all files will be copied
        file_extensions: List of extensions to include (e.g., ['.txt', '.jpg']).
                        If None, all files are copied.
    """
    os.makedirs(output_flat_folder, exist_ok=True)

    copied_count = 0
    skipped_count = 0
    conflict_count = 0

    for root, _, files in os.walk(source_root):
        for filename in files:
            # Filter by extension if specified
            if file_extensions:
                if not any(filename.lower().endswith(ext) for ext in file_extensions):
                    continue

            src_path = os.path.join(root, filename)

            # Create unique filename by prefixing with parent folder path
            rel_path = os.path.relpath(root, source_root).replace(os.sep, '_')
            if rel_path == ".":
                new_filename = filename
            else:
                new_filename = rel_path + "_" + filename

            dst_path = os.path.join(output_flat_folder, new_filename)

            # Check if file already exists
            if os.path.exists(dst_path):
                conflict_count += 1
                print(f"Warning: File already exists, skipping: {new_filename}")
                continue

            # Copy file to flattened folder
            shutil.copy2(src_path, dst_path)
            copied_count += 1

            if copied_count % 100 == 0:
                print(f"Copied {copied_count} files...")

    print(f"\n{'='*60}")
    print(f"Flattening complete!")
    print(f"Files copied: {copied_count}")
    print(f"Conflicts (skipped): {conflict_count}")
    print(f"Output folder: {output_flat_folder}")

    return copied_count, conflict_count


# Flatten the caption .txt files
flatten_folder(
  source_root="/content/drive/MyDrive/AML/Generated Images Captions",
  output_flat_folder="/content/drive/MyDrive/AML/Flattened Generated Images Captions",
  file_extensions=['.txt']  # Only copy .txt files
)

from google.colab import drive
drive.mount('/content/drive')

pip install clip-score

import shutil
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import multiprocessing as mp


def flatten_folder_fast(source_root, output_flat_folder, file_extensions=None, max_workers=None):
    """
    Flatten a nested folder structure into a single folder using parallel processing.

    Args:
        source_root: Root directory with nested structure to flatten
        output_flat_folder: Single folder where all files will be copied
        file_extensions: List of extensions to include (e.g., ['.txt', '.jpg']).
                        If None, all files are copied.
        max_workers: Number of parallel workers. If None, uses CPU count.
    """
    os.makedirs(output_flat_folder, exist_ok=True)

    if max_workers is None:
        max_workers = min(32, (mp.cpu_count() or 1) * 4)  # Good balance for I/O

    # Step 1: Collect all files to copy (fast scan)
    print("Scanning files...")
    files_to_copy = []

    for root, _, files in os.walk(source_root):
        for filename in files:
            # Filter by extension if specified
            if file_extensions:
                if not any(filename.lower().endswith(ext) for ext in file_extensions):
                    continue

            src_path = os.path.join(root, filename)

            # Create unique filename by prefixing with parent folder path
            rel_path = os.path.relpath(root, source_root).replace(os.sep, '_')
            if rel_path == ".":
                new_filename = filename
            else:
                new_filename = rel_path + "_" + filename

            dst_path = os.path.join(output_flat_folder, new_filename)

            # Check if file already exists
            if not os.path.exists(dst_path):
                files_to_copy.append((src_path, dst_path, new_filename))

    print(f"Found {len(files_to_copy)} files to copy")

    if len(files_to_copy) == 0:
        print("No files to copy!")
        return 0, 0

    # Step 2: Copy files in parallel
    copied_count = 0
    error_count = 0

    def copy_file(src_dst):
        """Copy a single file"""
        src, dst, filename = src_dst
        try:
            shutil.copy2(src, dst)
            return True, filename
        except Exception as e:
            return False, f"Error copying {filename}: {e}"

    print(f"Copying files with {max_workers} workers...")
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all copy tasks
        futures = {executor.submit(copy_file, file_info): file_info for file_info in files_to_copy}

        # Process results with progress bar
        with tqdm(total=len(files_to_copy), desc="Copying files") as pbar:
            for future in as_completed(futures):
                success, result = future.result()
                if success:
                    copied_count += 1
                else:
                    error_count += 1
                    print(f"\n{result}")
                pbar.update(1)

    print(f"\n{'='*60}")
    print(f"Flattening complete!")
    print(f"Files copied: {copied_count}")
    print(f"Errors: {error_count}")
    print(f"Output folder: {output_flat_folder}")
    print(f"{'='*60}")

    return copied_count, error_count

flatten_folder_fast(
  source_root="/content/drive/MyDrive/AML/Generated Images",
  output_flat_folder="/content/drive/MyDrive/AML/Flattened Generated Images V4",
  file_extensions=['.png']  # Only copy .txt files
)

import os

directory_path = "/root/.cache/kagglehub/datasets/dhananjayapaliwal/fulldataset/versions/3/temp_extraction/256x256/photo"

folders = [f for f in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, f))]

for folder in folders:
    print(folder)

flatten_folder_fast(
  source_root="/root/.cache/kagglehub/datasets/dhananjayapaliwal/fulldataset/versions/3/temp_extraction/256x256/photo",
  output_flat_folder="/content/drive/MyDrive/AML/Flattened Base Photos",
  file_extensions=['.png','.jpg','.jpeg']  # Only copy .txt files
)

import os
from PIL import Image
import torch
from transformers import CLIPProcessor, CLIPModel
from tqdm import tqdm
import csv
import numpy as np
from torch.utils.data import Dataset, DataLoader
from concurrent.futures import ThreadPoolExecutor
import multiprocessing as mp


# Set model and device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name = "openai/clip-vit-base-patch32"
print(f"Loading model on {device}...")
model = CLIPModel.from_pretrained(model_name).to(device)
processor = CLIPProcessor.from_pretrained(model_name)
model.eval()  # Set to eval mode once


class ImageCaptionDataset(Dataset):
    """Custom Dataset for efficient loading"""
    def __init__(self, image_paths, caption_paths, images_folder, captions_folder):
        self.image_paths = image_paths
        self.caption_paths = caption_paths
        self.images_folder = images_folder
        self.captions_folder = captions_folder

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = os.path.join(self.images_folder, self.image_paths[idx])
        cap_path = os.path.join(self.captions_folder, self.caption_paths[idx])

        image = Image.open(img_path).convert("RGB")
        with open(cap_path, 'r', encoding='utf-8') as f:
            caption = f.read().strip()

        return image, caption, self.image_paths[idx]


def collate_fn(batch):
    """Custom collate function for batching"""
    images, captions, filenames = zip(*batch)
    return list(images), list(captions), list(filenames)


def compute_clip_score_fast(dataloader, model, processor, device):
    """Optimized CLIP score computation with batching"""
    all_scores = []
    all_filenames = []

    with torch.no_grad():
        for images, captions, filenames in tqdm(dataloader, desc="Computing CLIP scores"):
            # Process batch with processor
            inputs = processor(
                text=captions,
                images=images,
                return_tensors="pt",
                padding=True,
                truncation=True
            )

            # Move to device
            inputs = {k: v.to(device) for k, v in inputs.items()}

            # Forward pass
            outputs = model(**inputs)
            logits_per_image = outputs.logits_per_image

            # Compute similarity scores (diagonal elements)
            batch_size = logits_per_image.shape[0]
            if batch_size == logits_per_image.shape[1]:
                # Square matrix - take diagonal
                probs = logits_per_image.softmax(dim=1)
                scores = torch.diag(probs)
            else:
                # Non-square - just take softmax of first column
                scores = logits_per_image.softmax(dim=1)[:, 0]

            all_scores.extend(scores.cpu().tolist())
            all_filenames.extend(filenames)

    return all_filenames, all_scores


def find_matching_pairs(images_folder, captions_folder, num_workers=4):
    """Find matching image-caption pairs in parallel"""
    print("Scanning for images...")

    # Find all image files
    image_paths = []
    for root, _, files in os.walk(images_folder):
        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                full_path = os.path.join(root, file)
                rel_path = os.path.relpath(full_path, images_folder)
                image_paths.append(rel_path)

    print(f"Found {len(image_paths)} images")
    print("Matching with captions...")

    # Match with captions
    def check_caption_exists(rel_image_path):
        base_name = os.path.splitext(rel_image_path)[0]
        rel_caption_path = base_name + ".txt"
        cap_full_path = os.path.join(captions_folder, rel_caption_path)
        if os.path.exists(cap_full_path):
            return (rel_image_path, rel_caption_path)
        return None

    # Use ThreadPoolExecutor for I/O-bound caption checking
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        results = list(tqdm(
            executor.map(check_caption_exists, image_paths),
            total=len(image_paths),
            desc="Finding caption pairs"
        ))

    # Filter out None results
    valid_pairs = [pair for pair in results if pair is not None]
    matched_images, matched_captions = zip(*valid_pairs) if valid_pairs else ([], [])

    print(f"Successfully matched {len(matched_images)} image-caption pairs")
    return list(matched_images), list(matched_captions)


def main(images_folder, captions_folder, output_csv_path, batch_size=32, num_workers=4):
    # Find matching pairs efficiently
    image_paths, caption_paths = find_matching_pairs(images_folder, captions_folder, num_workers)

    if len(image_paths) == 0:
        print("No image-caption pairs found. Please check your folder structure.")
        return

    # Create dataset and dataloader
    dataset = ImageCaptionDataset(image_paths, caption_paths, images_folder, captions_folder)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=True if torch.cuda.is_available() else False,
        prefetch_factor=2 if num_workers > 0 else None
    )

    # Calculate CLIP scores
    print(f"\nCalculating CLIP scores with batch_size={batch_size}...")
    filenames, clip_scores = compute_clip_score_fast(dataloader, model, processor, device)

    # Save to CSV
    print(f"\nSaving results to {output_csv_path}...")
    with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["image_filename", "clip_score"])
        for filename, score in zip(filenames, clip_scores):
            writer.writerow([filename, score])

    # Final Results
    clip_scores_array = np.array(clip_scores)
    print(f"\n{'='*60}")
    print(f"CLIP Score Statistics:")
    print(f"{'='*60}")
    print(f"Total pairs evaluated: {len(clip_scores)}")
    print(f"Average CLIP score: {np.mean(clip_scores_array):.4f}")
    print(f"Median CLIP score: {np.median(clip_scores_array):.4f}")
    print(f"Min CLIP score: {np.min(clip_scores_array):.4f}")
    print(f"Max CLIP score: {np.max(clip_scores_array):.4f}")
    print(f"Std CLIP score: {np.std(clip_scores_array):.4f}")
    print(f"{'='*60}")
    print(f"Results saved to: {output_csv_path}")


if __name__ == "__main__":
    images_folder = "/content/drive/MyDrive/AML/Generated Images"
    captions_folder = "/content/drive/MyDrive/AML/Generated Images Captions"
    output_csv_path = "/content/drive/MyDrive/AML/clip_scores.csv"

    # Adjust these parameters for your system
    # Larger batch_size = faster but more memory
    # More num_workers = faster I/O but more CPU/memory
    main(
        images_folder,
        captions_folder,
        output_csv_path,
        batch_size=64,  # Increase if you have more GPU memory
        num_workers=8   # Adjust based on CPU cores
    )

import csv

csv_path = "/content/drive/MyDrive/AML/clip_scores.csv"
with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(["image_filename", "clip_score"])  # CSV header
    for img_file, score in zip(image_files, clip_scores):
        writer.writerow([img_file, score])

print(f"CLIP scores saved to {csv_path}")